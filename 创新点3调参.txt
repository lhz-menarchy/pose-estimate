è¿™æ˜¯åˆ›æ–°ç‚¹3çš„æ¨¡å‹ä»£ç ï¼š
class MultiModalSemanticExtractor(nn.Module):
    """
    åŸºäºLLaVA-OneVision-1.5-4Bçš„å¤šæ¨¡æ€è¯­ä¹‰å…ˆéªŒæå–å™¨
    
    åŠŸèƒ½ï¼šèåˆè§†è§‰ç‰¹å¾å’Œè§¦è§‰ç‰¹å¾ï¼Œåˆ©ç”¨LLaVAæå–é«˜å±‚è¯­ä¹‰å…ˆéªŒ
    
    è¾“å…¥ï¼š
        - vis_feature: [B, 128] è§†è§‰ç‰¹å¾
        - tac_feature: [B, 128] è§¦è§‰ç‰¹å¾
    
    è¾“å‡ºï¼š
        - semantic_prior: [B, 768] è¯­ä¹‰å…ˆéªŒå‘é‡
    """
    
    def __init__(
        self,
        device,
        model_path="/home/sdc/lhz/SelectLSTM/llava-interleave-qwen-0.5b/",
        freeze_backbone=True,
        num_llm_layers=4
    ):
        """
        åˆå§‹åŒ–å¤šæ¨¡æ€è¯­ä¹‰å…ˆéªŒæå–å™¨
        
        Args:
            model_path: LLaVAæ¨¡å‹æœ¬åœ°è·¯å¾„
            freeze_backbone: æ˜¯å¦å†»ç»“LLaVAä¸»å¹²ç½‘ç»œ
            num_llm_layers: ä½¿ç”¨çš„LLM transformerå±‚æ•°
        """
        super().__init__()
        
        self.device = device
        self.freeze_backbone = freeze_backbone
        self.num_llm_layers = num_llm_layers
        
        print("\n" + "="*80)
        print("åˆå§‹åŒ–åŸºäºLLaVA-OneVision-0.5Bçš„å¤šæ¨¡æ€è¯­ä¹‰å…ˆéªŒæå–å™¨".center(80))
        print("="*80)
        
        # åŠ è½½LLaVAæ¨¡å‹
        self._load_llava(model_path)
        
        # æ„å»ºç‰¹å¾æŠ•å½±å±‚ (128 -> hidden_size)
        self.visual_projector = self._build_projector(128, self.hidden_size)
        self.tactile_projector = self._build_projector(128, self.hidden_size)
        
        # æ„å»ºè·¨æ¨¡æ€èåˆå±‚
        self.cross_modal_fusion = self._build_fusion_layer()
        
        # æ„å»ºè¯­ä¹‰ç”Ÿæˆå™¨ (hidden_size -> 768)
        self.semantic_head = self._build_semantic_head()
        
        # é…ç½®è®­ç»ƒæ¨¡å¼
        self._setup_training_mode()
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self._print_statistics()
        print("="*80 + "\n")
    
    def _load_llava(self, model_path):
        """åŠ è½½LLaVA-OneVisionæ¨¡å‹"""
        print("\n[1/4] åŠ è½½LLaVA-OneVision-0.5Bæ¨¡å‹...")
        
        try:
            self.llava_model = LlavaForConditionalGeneration.from_pretrained(
                model_path, 
                torch_dtype=torch.float32, 
                local_files_only=True
            ).to(self.device)
            
            # è·å–è¯­è¨€æ¨¡å‹çš„éšè—ç»´åº¦
            self.hidden_size = self.llava_model.config.text_config.hidden_size
            
            print(f"      âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"      - éšè—ç»´åº¦: {self.hidden_size}")
            print(f"      - è®¾å¤‡: {self.device}")
            
        except Exception as e:
            raise RuntimeError(f"åŠ è½½LLaVAæ¨¡å‹å¤±è´¥: {e}")
    
    def _build_projector(self, input_dim, output_dim):
        """æ„å»ºç‰¹å¾æŠ•å½±å™¨"""
        return nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.LayerNorm(256),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(256, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(512, output_dim),
            nn.LayerNorm(output_dim)
        ).to(self.device)
    
    def _build_fusion_layer(self):
        """æ„å»ºè·¨æ¨¡æ€èåˆå±‚"""
        print("\n[2/4] æ„å»ºè·¨æ¨¡æ€èåˆå±‚...")
        
        fusion = nn.ModuleDict({
            'attention': nn.MultiheadAttention(
                embed_dim=self.hidden_size,
                num_heads=8,
                dropout=0.1,
                batch_first=True
            ),
            'ffn': nn.Sequential(
                nn.Linear(self.hidden_size, self.hidden_size * 4),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(self.hidden_size * 4, self.hidden_size)
            ),
            'norm1': nn.LayerNorm(self.hidden_size),
            'norm2': nn.LayerNorm(self.hidden_size)
        }).to(self.device)
        
        print(f"      âœ“ èåˆå±‚æ„å»ºå®Œæˆ (8-head attention)")
        return fusion
    
    def _build_semantic_head(self):
        """æ„å»ºè¯­ä¹‰å…ˆéªŒç”Ÿæˆå™¨"""
        print("\n[3/4] æ„å»ºè¯­ä¹‰å…ˆéªŒç”Ÿæˆå™¨...")
        
        head = nn.Sequential(
            nn.Linear(self.hidden_size, 1536),
            nn.LayerNorm(1536),
            nn.GELU(),
            nn.Dropout(0.15),
            nn.Linear(1536, 1024),
            nn.LayerNorm(1024),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(1024, 768),
            nn.LayerNorm(768)
        ).to(self.device)
        
        print(f"      âœ“ è¯­ä¹‰ç”Ÿæˆå™¨æ„å»ºå®Œæˆ ({self.hidden_size} -> 768)")
        return head
    
    def _setup_training_mode(self):
        """é…ç½®è®­ç»ƒæ¨¡å¼"""
        print("\n[4/4] é…ç½®è®­ç»ƒæ¨¡å¼...")
        
        if self.freeze_backbone:
            for param in self.llava_model.parameters():
                param.requires_grad = False
            self.llava_model.eval()
            print(f"      âœ“ LLaVAä¸»å¹²å·²å†»ç»“")
        else:
            for param in self.llava_model.parameters():
                param.requires_grad = True
            self.llava_model.train()
            print(f"      âœ“ LLaVAä¸»å¹²å¯è®­ç»ƒ")
    
    def _print_statistics(self):
        """æ‰“å°å‚æ•°ç»Ÿè®¡ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        print("\n" + "-"*80)
        print(f"âœ“ åˆå§‹åŒ–å®Œæˆ".center(80))
        print(f"  æ€»å‚æ•°é‡: {total_params:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)")
        print("-"*80)
    
    def _extract_llm_features(self, feature_sequence):
        """
        ä½¿ç”¨LLaVAçš„è¯­è¨€æ¨¡å‹æå–æ·±å±‚è¯­ä¹‰ç‰¹å¾
        
        Args:
            feature_sequence: [B, 2, hidden_size] åŒæ¨¡æ€ç‰¹å¾åºåˆ—
        
        Returns:
            semantic_features: [B, hidden_size] è¯­ä¹‰ç‰¹å¾
        """
        # è·å–è¯­è¨€æ¨¡å‹çš„transformerå±‚
        language_model = self.llava_model.language_model
        
        if hasattr(language_model, 'model') and hasattr(language_model.model, 'layers'):
            layers = language_model.model.layers
        elif hasattr(language_model, 'layers'):
            layers = language_model.layers
        else:
            warnings.warn("æ— æ³•å®šä½Transformerå±‚ï¼Œä½¿ç”¨å…¨å±€å¹³å‡æ± åŒ–")
            return feature_sequence.mean(dim=1)
        
        # é€šè¿‡å‰Nå±‚Transformeræå–è¯­ä¹‰
        hidden_states = feature_sequence
        num_layers = min(self.num_llm_layers, len(layers))
        
        for i in range(num_layers):
            layer = layers[i]
            try:
                if self.freeze_backbone:
                    with torch.no_grad():
                        output = layer(hidden_states)
                else:
                    output = layer(hidden_states)
                
                # æå–hidden states
                hidden_states = output[0] if isinstance(output, tuple) else output
                
            except Exception as e:
                warnings.warn(f"Layer {i} å¤„ç†å¤±è´¥: {e}")
                break
        
        # å…¨å±€å¹³å‡æ± åŒ–
        semantic_features = hidden_states.mean(dim=1)  # [B, hidden_size]
        
        return semantic_features
    
    def forward(self, vis_feature, tac_feature):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            vis_feature: [B, 128] è§†è§‰ç‰¹å¾
            tac_feature: [B, 128] è§¦è§‰ç‰¹å¾
        
        Returns:
            semantic_prior: [B, 768] è¯­ä¹‰å…ˆéªŒå‘é‡
        """
        # Step 1: ç‰¹å¾æŠ•å½±
        vis_embedded = self.visual_projector(vis_feature)      # [B, hidden_size]
        tac_embedded = self.tactile_projector(tac_feature)     # [B, hidden_size]
        
        # Step 2: æ„å»ºåŒæ¨¡æ€ç‰¹å¾åºåˆ—
        feature_seq = torch.stack([vis_embedded, tac_embedded], dim=1)  # [B, 2, hidden_size]
        
        # Step 3: è·¨æ¨¡æ€æ³¨æ„åŠ›èåˆ
        attn_output, _ = self.cross_modal_fusion['attention'](
            feature_seq, feature_seq, feature_seq
        )
        feature_seq = self.cross_modal_fusion['norm1'](feature_seq + attn_output)
        
        # å‰é¦ˆç½‘ç»œ
        ffn_output = self.cross_modal_fusion['ffn'](feature_seq)
        feature_seq = self.cross_modal_fusion['norm2'](feature_seq + ffn_output)
        
        # Step 4: ä½¿ç”¨LLaVAçš„LLMæå–æ·±å±‚è¯­ä¹‰
        semantic_features = self._extract_llm_features(feature_seq)  # [B, hidden_size]
        
        # Step 5: ç”Ÿæˆ768ç»´è¯­ä¹‰å…ˆéªŒ
        semantic_prior = self.semantic_head(semantic_features)  # [B, 768]
        
        return semantic_prior




# ==================== CSAå› æœè¯­ä¹‰å¯¹é½æ¨¡å— ====================
class CausalSemanticAlignment(nn.Module):
    """
    CSA (Causal Semantic Alignment) å› æœè¯­ä¹‰å¯¹é½æ¨¡å—
    æ ¸å¿ƒåˆ›æ–°ï¼šé€šè¿‡åäº‹å®æ¨ç†æ¶ˆé™¤æ¨¡æ€åè§ï¼Œå®ç°å› æœå±‚é¢çš„ç‰¹å¾å¯¹é½
    """
    def __init__(self, vis_dim=128, tac_dim=128, scene_dim=128, 
                 semantic_dim=768, hidden_dim=256, num_heads=4):
        super().__init__()
        
        # ç‰¹å¾æŠ•å½±å±‚
        self.vis_proj = nn.Linear(vis_dim, hidden_dim)
        self.tac_proj = nn.Linear(tac_dim, hidden_dim)
        self.semantic_proj = nn.Linear(semantic_dim, hidden_dim)
        
        # å› æœå¹²é¢„æ¨¡å—ï¼šåäº‹å®ç‰¹å¾ç”Ÿæˆ
        self.counterfactual_generator = CounterfactualGenerator(
            hidden_dim=hidden_dim, 
            num_heads=num_heads
        )
        
        # æ··æ·†å› å­è§£è€¦å™¨
        self.confounder_disentangler = ConfounderDisentangler(
            feature_dim=hidden_dim
        )
        
        # å› æœæ•ˆåº”ä¼°è®¡å™¨
        self.causal_effect_estimator = CausalEffectEstimator(
            feature_dim=hidden_dim
        )
        
        # æœ€ç»ˆèåˆå±‚
        self.causal_fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
    def forward(self, vis_feature, tac_feature, semantic_prior):
        """
        å› æœå¯¹é½å‰å‘ä¼ æ’­
        Args:
            vis_feature: [B, 128] è§†è§‰ç‰¹å¾
            tac_feature: [B, 128] è§¦è§‰ç‰¹å¾
            semantic_prior: [B, 768] è¯­ä¹‰å…ˆéªŒ
        Returns:
            causal_feature: [B, 256] å› æœæ¶ˆååçš„èåˆç‰¹å¾
            causal_info: dict å› æœåˆ†æä¿¡æ¯ï¼ˆç”¨äºå¯è§†åŒ–/æŸå¤±è®¡ç®—ï¼‰
        """
        B = vis_feature.shape[0]
        
        # ç‰¹å¾æŠ•å½±åˆ°ç»Ÿä¸€ç©ºé—´
        vis_h = self.vis_proj(vis_feature)  # [B, hidden_dim]
        tac_h = self.tac_proj(tac_feature)  # [B, hidden_dim]
        sem_h = self.semantic_proj(semantic_prior)  # [B, hidden_dim]
        
        # æ­¥éª¤1: æ··æ·†å› å­è§£è€¦
        vis_causal, vis_spurious = self.confounder_disentangler(vis_h, sem_h)
        tac_causal, tac_spurious = self.confounder_disentangler(tac_h, sem_h)
        
        # æ­¥éª¤2: åäº‹å®ç‰¹å¾ç”Ÿæˆï¼ˆå¹²é¢„æ“ä½œï¼‰
        # do(V=v') - å¹²é¢„è§†è§‰æ¨¡æ€
        vis_counterfactual = self.counterfactual_generator(
            vis_causal, tac_causal, intervention_type='vision'
        )
        
        # do(T=t') - å¹²é¢„è§¦è§‰æ¨¡æ€  
        tac_counterfactual = self.counterfactual_generator(
            tac_causal, vis_causal, intervention_type='tactile'
        )
        
        # æ­¥éª¤3: å› æœæ•ˆåº”ä¼°è®¡
        # è®¡ç®—ç›´æ¥å› æœæ•ˆåº” (DCE)
        vis_direct_effect = self.causal_effect_estimator(
            vis_causal, vis_counterfactual
        )
        tac_direct_effect = self.causal_effect_estimator(
            tac_causal, tac_counterfactual
        )
        
        # æ­¥éª¤4: å› æœç‰¹å¾èåˆ
        causal_combined = torch.cat([vis_direct_effect, tac_direct_effect], dim=-1)
        causal_feature = self.causal_fusion(causal_combined)  # [B, hidden_dim]
        
        # æ”¶é›†å› æœåˆ†æä¿¡æ¯ï¼ˆç”¨äºæŸå¤±å‡½æ•°å’Œå¯è§†åŒ–ï¼‰
        causal_info = {
            'vis_spurious': vis_spurious,
            'tac_spurious': tac_spurious,
            'vis_causal': vis_causal,
            'tac_causal': tac_causal,
            'counterfactual_gap_vis': torch.norm(vis_causal - vis_counterfactual, dim=-1).mean(),
            'counterfactual_gap_tac': torch.norm(tac_causal - tac_counterfactual, dim=-1).mean()
        }
        
        return causal_feature, causal_info


class CounterfactualGenerator(nn.Module):
    """åäº‹å®ç‰¹å¾ç”Ÿæˆå™¨ - å®ç°doç®—å­çš„å¹²é¢„æ•ˆæœ"""
    def __init__(self, hidden_dim=256, num_heads=4):
        super().__init__()
        
        # äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼šæ¨¡æ‹Ÿå› æœå¹²é¢„
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads,
            batch_first=True
        )
        
        # å¹²é¢„å¼ºåº¦è°ƒèŠ‚å™¨
        self.intervention_gate = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.Sigmoid()
        )
        
    def forward(self, target_feature, context_feature, intervention_type='vision'):
        """
        ç”Ÿæˆåäº‹å®ç‰¹å¾
        Args:
            target_feature: [B, D] è¢«å¹²é¢„çš„æ¨¡æ€ç‰¹å¾
            context_feature: [B, D] ä¸Šä¸‹æ–‡æ¨¡æ€ç‰¹å¾
        Returns:
            counterfactual: [B, D] åäº‹å®ç‰¹å¾
        """
        B, D = target_feature.shape
        
        # æ‰©å±•ç»´åº¦ä»¥é€‚é…MultiheadAttention
        target = target_feature.unsqueeze(1)  # [B, 1, D]
        context = context_feature.unsqueeze(1)  # [B, 1, D]
        
        # äº¤å‰æ³¨æ„åŠ›ï¼šä»ä¸Šä¸‹æ–‡ä¸­"å€Ÿç”¨"ä¿¡æ¯æ¨¡æ‹Ÿå¹²é¢„
        intervened, _ = self.cross_attention(
            query=target,
            key=context,
            value=context
        )  # [B, 1, D]
        
        intervened = intervened.squeeze(1)  # [B, D]
        
        # è®¡ç®—å¹²é¢„å¼ºåº¦ï¼ˆè‡ªé€‚åº”ï¼‰
        gate_input = torch.cat([target_feature, context_feature], dim=-1)
        gate = self.intervention_gate(gate_input)  # [B, D]
        
        # åŠ æƒæ··åˆåŸå§‹ç‰¹å¾å’Œå¹²é¢„ç‰¹å¾
        counterfactual = gate * intervened + (1 - gate) * target_feature
        
        return counterfactual


class ConfounderDisentangler(nn.Module):
    """æ··æ·†å› å­è§£è€¦å™¨ - åˆ†ç¦»å› æœç‰¹å¾å’Œè™šå‡ç›¸å…³ç‰¹å¾"""
    def __init__(self, feature_dim=256):
        super().__init__()
        
        # å› æœç¼–ç å™¨
        self.causal_encoder = nn.Sequential(
            nn.Linear(feature_dim, feature_dim),
            nn.ReLU(),
            nn.Linear(feature_dim, feature_dim)
        )
        
        # è™šå‡ç›¸å…³ç¼–ç å™¨
        self.spurious_encoder = nn.Sequential(
            nn.Linear(feature_dim, feature_dim),
            nn.ReLU(),
            nn.Linear(feature_dim, feature_dim)
        )
        
        # è¯­ä¹‰å¼•å¯¼æ³¨æ„åŠ›ï¼ˆåˆ©ç”¨å¤§æ¨¡å‹å…ˆéªŒæŒ‡å¯¼è§£è€¦ï¼‰
        self.semantic_guide = nn.Sequential(
            nn.Linear(feature_dim * 2, feature_dim),
            nn.Tanh()
        )
        
    def forward(self, feature, semantic_prior):
        """
        è§£è€¦å› æœå’Œè™šå‡ç›¸å…³ç‰¹å¾
        Args:
            feature: [B, D] æ¨¡æ€ç‰¹å¾
            semantic_prior: [B, D] è¯­ä¹‰å…ˆéªŒ
        Returns:
            causal_feat: [B, D] å› æœç‰¹å¾
            spurious_feat: [B, D] è™šå‡ç‰¹å¾
        """
        # è¯­ä¹‰å¼•å¯¼çš„æ³¨æ„åŠ›æƒé‡
        guide_input = torch.cat([feature, semantic_prior], dim=-1)
        guide_weight = self.semantic_guide(guide_input)  # [B, D]
        
        # åŠ æƒåç¼–ç 
        guided_feature = feature * (1 + guide_weight)
        
        causal_feat = self.causal_encoder(guided_feature)
        spurious_feat = self.spurious_encoder(feature - guided_feature)
        
        # æ­£äº¤åŒ–çº¦æŸï¼ˆç¡®ä¿è§£è€¦ï¼‰
        causal_feat = causal_feat - torch.sum(causal_feat * spurious_feat, dim=-1, keepdim=True) * spurious_feat
        
        return causal_feat, spurious_feat


class CausalEffectEstimator(nn.Module):
    """å› æœæ•ˆåº”ä¼°è®¡å™¨ - é‡åŒ–ç›´æ¥å› æœæ•ˆåº”"""
    def __init__(self, feature_dim=256):
        super().__init__()
        
        self.effect_net = nn.Sequential(
            nn.Linear(feature_dim * 2, feature_dim),
            nn.ReLU(),
            nn.Linear(feature_dim, feature_dim)
        )
        
    def forward(self, factual, counterfactual):
        """
        è®¡ç®—å› æœæ•ˆåº” = E[Y|do(X=x)] - E[Y|do(X=x')]
        Args:
            factual: [B, D] äº‹å®ç‰¹å¾
            counterfactual: [B, D] åäº‹å®ç‰¹å¾
        Returns:
            causal_effect: [B, D] å› æœæ•ˆåº”ç‰¹å¾
        """
        diff = factual - counterfactual
        combined = torch.cat([factual, diff], dim=-1)
        
        causal_effect = self.effect_net(combined)
        return causal_effect


# ==================== ä¸»æ¨¡å‹ ====================
class CausalMultiModalPoseEstimator(nn.Module):
    """å› æœå¤šæ¨¡æ€å§¿åŠ¿ä¼°è®¡ä¸»æ¨¡å‹"""
    def __init__(self, Flag_Merge=False, device=None):
        super().__init__()
        self.Flag_Merge = Flag_Merge
        self.device = device if device else torch.device('cuda:0')
        
        # åŸºç¡€ç‰¹å¾ç¼–ç å™¨
        self.imageCNN = ImageCNN(Flag_Merge=True).to(self.device)
        self.tactileGCN = TactileGCN(Flag_Merge=True).to(self.device)
        
        # # åœºæ™¯å› å­ç¼–ç å™¨
        # self.scene_encoder = SceneFactorEncoder().to(self.device)
        
        # å¤šæ¨¡æ€è¯­ä¹‰æå–å™¨
        self.semantic_extractor = MultiModalSemanticExtractor(device=self.device)
        
        # CSAå› æœåˆ†ææ¨¡å—
        self.csa = CausalSemanticAlignment(
            vis_dim=128, 
            tac_dim=128, 
            scene_dim=128, 
            semantic_dim=768,
            hidden_dim=256
        ).to(self.device)
        
        # å§¿åŠ¿å›å½’å¤´
        self.pose_regressor = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 7)  # [x, y, z, qx, qy, qz, qw]
        ).to(self.device)
        
        # å¯é€‰èåˆç‰¹å¾è¾“å‡º
        self.fusion_feature = nn.Linear(256, 256).to(self.device)
        
        self.dropout = nn.Dropout(p=0.1)
        
    def forward(self, rgbd_data, tactile_data):
        """
        å‰å‘ä¼ æ’­
        Args:
            rgbd_data: [B, 4, H, W] RGBDå›¾åƒ
            tactile_data: [B, N, C] è§¦è§‰å›¾æ•°æ®
        Returns:
            predict_pose: [B, 7] é¢„æµ‹çš„6Då§¿åŠ¿
            å…¶ä»–è¾“å‡ºå–å†³äºFlag_Merge
        """
        # åŸºç¡€ç‰¹å¾æå–
        _, vis_feature = self.imageCNN(rgbd_data)  # [B, 128]
        _, tac_feature = self.tactileGCN(tactile_data)  # [B, 128]
        
        # **æ ¸å¿ƒï¼šå¤šæ¨¡æ€å¤§æ¨¡å‹è¯­ä¹‰å…ˆéªŒæå–**
        semantic_prior = self.semantic_extractor(
            vis_feature, tac_feature
        )  # [B, 768]
        
        # Dropoutæ­£åˆ™åŒ–
        vis_feature = self.dropout(vis_feature)
        tac_feature = self.dropout(tac_feature)
        
        # **æ ¸å¿ƒï¼šCSAå› æœæ¶ˆåå¤„ç†**
        causal_feature, causal_info = self.csa(
            vis_feature, tac_feature, semantic_prior
        )  # [B, 256]
        
        # æœ€ç»ˆå§¿æ€é¢„æµ‹
        predict_pose = self.pose_regressor(causal_feature)  # [B, 7]
        
        if self.Flag_Merge:
            fusion_feat = self.fusion_feature(causal_feature)
            return predict_pose, fusion_feat
        else:
            return predict_pose
è¿™æ˜¯åˆ›æ–°ç‚¹3çš„è®­ç»ƒä»£ç ï¼š
############################åˆ›æ–°ç‚¹3è®­ç»ƒ:å› æœæ¶ˆå+MLLM (ä¼˜åŒ–ç‰ˆ)#############################
############################ç­–ç•¥C: ä¸¤é˜¶æ®µè®­ç»ƒå®Œæ•´ä»£ç #############################
# ========== æ–°å¢ä»£ç å¼€å§‹ ==========
from collections import deque

class SimpleAdaptiveWeights:
    def __init__(self):
        self.trans_hist = deque(maxlen=5)
        self.rot_hist = deque(maxlen=5)
        
        # ğŸ”¥ ä¿®æ”¹1ï¼šå¤§å¹…æé«˜ä½ç½®åˆå§‹æƒé‡
        self.trans_w = 2.0   # åŸæ¥æ˜¯ 1.2
        self.rot_w = 0.6     # åŸæ¥æ˜¯ 1.0
    
    def update(self, trans_loss, rot_loss):
        self.trans_hist.append(trans_loss)
        self.rot_hist.append(rot_loss)
        
        if len(self.trans_hist) == 5:
            avg_trans = sum(self.trans_hist) / 5
            avg_rot = sum(self.rot_hist) / 5
            ratio = avg_rot / (avg_trans + 1e-8)
            
            # ğŸ”¥ ä¿®æ”¹2ï¼šæ›´æ¿€è¿›åœ°å¢åŠ ä½ç½®æƒé‡
            if ratio > 1.5:  # åŸæ¥æ˜¯ 2.0
                self.trans_w = min(self.trans_w * 1.15, 5.0)  # ä¸Šé™ä»3.0æé«˜åˆ°5.0
                self.rot_w = max(self.rot_w * 0.9, 0.3)       # ä¸‹é™ä»0.5é™åˆ°0.3
            elif ratio < 0.3:  # åŸæ¥æ˜¯ 0.5
                self.trans_w = max(self.trans_w * 0.95, 1.5)  # ä¸‹é™ä»0.5æé«˜åˆ°1.5
                self.rot_w = min(self.rot_w * 1.1, 1.5)
        
        return self.trans_w, self.rot_w
# ========== æ–°å¢ä»£ç ç»“æŸ ==========
def train_causal_multimodal_two_stage(object_name):
    """
    ç­–ç•¥C: ä¸¤é˜¶æ®µè®­ç»ƒ - å…ˆé¢„è®­ç»ƒè¯­ä¹‰å¯¹é½ï¼Œå†å¾®è°ƒå› æœæ¨ç†
    
    é€‚ç”¨åœºæ™¯: æ•°æ®é‡>5000ï¼Œè¿½æ±‚æè‡´æ€§èƒ½
    è®­ç»ƒæµç¨‹:
        é˜¶æ®µ1 (50 epochs): é¢„è®­ç»ƒè¯­ä¹‰æå–å™¨å’Œæ¨¡æ€å¯¹é½
            - è®­ç»ƒç›®æ ‡: å­¦ä¹ é«˜è´¨é‡çš„è·¨æ¨¡æ€è¯­ä¹‰è¡¨ç¤º
            - å†»ç»“æ¨¡å—: å› æœæ¨¡å—CSAã€ä»»åŠ¡å¤´
            - è®­ç»ƒæ¨¡å—: ç‰¹å¾ç¼–ç å™¨ã€è¯­ä¹‰æå–å™¨
        
        é˜¶æ®µ2 (100 epochs): å¾®è°ƒå› æœæ¨ç†å’Œå§¿æ€ä¼°è®¡
            - è®­ç»ƒç›®æ ‡: åŸºäºé¢„è®­ç»ƒè¯­ä¹‰å­¦ä¹ å› æœæ¨ç†
            - å†»ç»“æ¨¡å—: è¯­ä¹‰æå–å™¨(å¯é€‰)
            - è®­ç»ƒæ¨¡å—: å› æœæ¨¡å—CSAã€ä»»åŠ¡å¤´
    """
    print(f"\n{'='*80}")
    print(f"å¯åŠ¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥: {object_name}".center(80))
    print(f"{'='*80}\n")
    
    # ==================== 1. æ•°æ®å‡†å¤‡ ====================
    dataset_path = os.path.join(WholeDatasetPath, object_name)
    object_name = dataset_path.split('/')[-1]
    
    # è¶…å‚æ•°é…ç½®
    config = {
        # æ•°æ®é…ç½®
        'batch_size': 32,
        'num_workers': 16,
        'split_rate': 0.6,
        
        # é˜¶æ®µ1é…ç½®
        'stage1_epochs': 50,
        'stage1_lr': 2e-4,
        'stage1_weight_decay': 0.01,
        
        # é˜¶æ®µ2é…ç½®
        'stage2_epochs': 100,
        'stage2_lr': 5e-4,
        'stage2_weight_decay': 0.02,
        'freeze_semantic_stage2': True,  # é˜¶æ®µ2æ˜¯å¦å†»ç»“è¯­ä¹‰æå–å™¨
        'stage2_llm_unfreeze_epoch': 30,  # é˜¶æ®µ2ä½•æ—¶è§£å†»LLM
        
        # è®­ç»ƒé…ç½®
        'patience': 20,
        'use_amp': True,  # æ··åˆç²¾åº¦è®­ç»ƒ
    }
    
    # æ•°æ®é›†åŠ è½½
    train_dataset = MergeDataset(dataset_path, split_method='shuffle', 
                                split='train', split_rate=config['split_rate'])
    test_dataset = MergeDataset(dataset_path, split_method='shuffle', 
                               split='test', split_rate=config['split_rate'])
    
    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], 
                            shuffle=True, num_workers=config['num_workers'],
                            pin_memory=True, drop_last=True)
    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], 
                           shuffle=False, num_workers=config['num_workers'],
                           pin_memory=True)
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")
    print(f"è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}")
    
    # ==================== 2. æ¨¡å‹åˆå§‹åŒ– ====================
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    network = CausalMultiModalPoseEstimator(Flag_Merge=True, device=device).to(device)
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = GradScaler() if config['use_amp'] else None
    
    # æŸå¤±å‡½æ•°å‡†å¤‡
    object_point_cloud = o3d.io.read_triangle_mesh(
        os.path.join(Abs_Path, f"ycb_models/{object_name}/textured.obj")
    )
    object_point_cloud = object_point_cloud.sample_points_uniformly(3000)
    loss6D = Loss6D(o3d_point_cloud=object_point_cloud, num_points=3000)
    
    edge_index = torch.tensor([[0,1,2,3,4],[5,5,5,5,5]], dtype=torch.long).to(device)
    
    # ========================================================
    # é˜¶æ®µ1: è¯­ä¹‰é¢„è®­ç»ƒ
    # ========================================================
    print("\n" + "="*80)
    print("é˜¶æ®µ1: è¯­ä¹‰é¢„è®­ç»ƒ - è®­ç»ƒè¯­ä¹‰æå–å™¨å’Œæ¨¡æ€å¯¹é½".center(80))
    print("="*80 + "\n")
    
    # é˜¶æ®µ1æ‰§è¡Œ
    best_semantic_path = train_stage1_semantic_pretraining(
        network=network,
        train_loader=train_loader,
        test_loader=test_loader,
        edge_index=edge_index,
        config=config,
        device=device,
        scaler=scaler
    )
    
    print("\n" + "="*80)
    print("é˜¶æ®µ1å®Œæˆ - è¯­ä¹‰æå–å™¨é¢„è®­ç»ƒå®Œæ¯•".center(80))
    print("="*80 + "\n")
    
    # ========================================================
    # é˜¶æ®µ2: å› æœå¾®è°ƒ
    # ========================================================
    print("\n" + "="*80)
    print("é˜¶æ®µ2: å› æœå¾®è°ƒ - è®­ç»ƒå› æœæ¨¡å—å’Œä»»åŠ¡å¤´".center(80))
    print("="*80 + "\n")
    
    # åŠ è½½æœ€ä½³è¯­ä¹‰æ¨¡å‹
    print(f"åŠ è½½é¢„è®­ç»ƒè¯­ä¹‰æ¨¡å‹: {best_semantic_path}")
    checkpoint = torch.load(best_semantic_path)
    network.semantic_extractor.load_state_dict(checkpoint['semantic_extractor'])
    network.imageCNN.load_state_dict(checkpoint['imageCNN'])
    network.tactileGCN.load_state_dict(checkpoint['tactileGCN'])
    all_logger_info("âœ“ é¢„è®­ç»ƒè¯­ä¹‰æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # é˜¶æ®µ2æ‰§è¡Œ
    best_loss = train_stage2_causal_finetuning(
        network=network,
        train_loader=train_loader,
        test_loader=test_loader,
        edge_index=edge_index,
        loss6D=loss6D,
        config=config,
        device=device,
        scaler=scaler
    )
    
    print("\n" + "="*80)
    print(f"ä¸¤é˜¶æ®µè®­ç»ƒå®Œæˆ: {object_name} | æœ€ä½³æŸå¤±: {best_loss:.6f}".center(80))
    print("="*80 + "\n")


# ==================== é˜¶æ®µ1: è¯­ä¹‰é¢„è®­ç»ƒ ====================
def train_stage1_semantic_pretraining(network, train_loader, test_loader, 
                                     edge_index, config, device, scaler):
    """
    é˜¶æ®µ1: è¯­ä¹‰é¢„è®­ç»ƒ
    ç›®æ ‡: è®­ç»ƒè¯­ä¹‰æå–å™¨å­¦ä¹ é«˜è´¨é‡çš„è·¨æ¨¡æ€è¯­ä¹‰è¡¨ç¤º
    """
    print("[Stage1] é…ç½®è®­ç»ƒå‚æ•°...")
    
    # å†»ç»“å› æœæ¨¡å—å’Œä»»åŠ¡å¤´
    for param in network.csa.parameters():
        param.requires_grad = False
    for param in network.pose_regressor.parameters():
        param.requires_grad = False
    for param in network.fusion_feature.parameters():
        param.requires_grad = False
    
    all_logger_info("âœ“ å› æœæ¨¡å—å’Œä»»åŠ¡å¤´å·²å†»ç»“")
    
    # ä¼˜åŒ–å™¨ï¼šåªä¼˜åŒ–ç‰¹å¾ç¼–ç å™¨å’Œè¯­ä¹‰æå–å™¨
    trainable_params = [
        {'params': network.imageCNN.parameters(), 
         'lr': config['stage1_lr'] * 0.5, 'name': 'imageCNN'},
        {'params': network.tactileGCN.parameters(), 
         'lr': config['stage1_lr'] * 0.5, 'name': 'tactileGCN'},
        {'params': network.semantic_extractor.visual_projector.parameters(), 
         'lr': config['stage1_lr'], 'name': 'vis_proj'},
        {'params': network.semantic_extractor.tactile_projector.parameters(), 
         'lr': config['stage1_lr'], 'name': 'tac_proj'},
        {'params': network.semantic_extractor.cross_modal_fusion.parameters(), 
         'lr': config['stage1_lr'], 'name': 'fusion'},
        {'params': network.semantic_extractor.semantic_head.parameters(), 
         'lr': config['stage1_lr'], 'name': 'sem_head'},
    ]
    
    optimizer = torch.optim.AdamW(trainable_params, 
                                 weight_decay=config['stage1_weight_decay'],
                                 betas=(0.9, 0.999), eps=1e-8)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=config['stage1_epochs'], eta_min=1e-6
    )
    
    # è®­ç»ƒç›‘æ§
    best_loss = float('inf')
    patience_counter = 0
    
    # è¯­ä¹‰è´¨é‡è¯„ä¼°å™¨
    semantic_reconstructor = SemanticReconstructor(
        semantic_dim=768, feature_dim=128
    ).to(device)
    optimizer_recon = torch.optim.Adam(semantic_reconstructor.parameters(), lr=1e-3)
    
    # ==================== è®­ç»ƒå¾ªç¯ ====================
    for epoch in range(config['stage1_epochs']):
        network.train()
        semantic_reconstructor.train()
        
        epoch_losses = {
            'contrast': 0.0,
            'reconstruction': 0.0,
            'diversity': 0.0,
            'quality': 0.0,
            'total': 0.0
        }
        
        for batch_idx, data in enumerate(train_loader):
            tip_pose_array, tip_data_array, rgbd_data, target_pose = data
            tip_pose_array = tip_pose_array.to(device, non_blocking=True)
            tip_data_array = tip_data_array.to(device, non_blocking=True)
            rgbd_data = rgbd_data.to(device, non_blocking=True)
            target_pose = target_pose.to(device, non_blocking=True)
            
            # æ„å»ºå›¾æ•°æ®
            batch_graphdata_list = []
            for i, tip_pose in enumerate(tip_pose_array):
                tactile_data = tip_data_array[i]
                graphData = GraphData(x=tip_pose, edge_index=edge_index, 
                                    edge_attr=tactile_data)
                batch_graphdata_list.append(graphData)
            input_tactile_data = Batch.from_data_list(batch_graphdata_list)
            
            optimizer.zero_grad()
            optimizer_recon.zero_grad()
            
            # å‰å‘ä¼ æ’­
            with autocast(enabled=config['use_amp']):
                _, vis_feat = network.imageCNN(rgbd_data)
                _, tac_feat = network.tactileGCN(input_tactile_data)
                semantic_prior = network.semantic_extractor(vis_feat, tac_feat)
                
                # æŸå¤±1: æ¨¡æ€å¯¹æ¯”æŸå¤± (ä¸»æŸå¤±, æƒé‡1.0)
                loss_contrast = compute_contrastive_loss(
                    vis_feat, tac_feat, temperature=0.07
                )
                
                # æŸå¤±2: è¯­ä¹‰é‡æ„æŸå¤± (ç¡®ä¿ä¿¡æ¯ä¿ç•™, æƒé‡0.3)
                vis_recon, tac_recon = semantic_reconstructor(semantic_prior)
                loss_recon = (F.mse_loss(vis_recon, vis_feat) + 
                            F.mse_loss(tac_recon, tac_feat)) / 2
                
                # æŸå¤±3: è¯­ä¹‰å¤šæ ·æ€§æŸå¤± (é˜²æ­¢mode collapse, æƒé‡0.2)
                loss_diversity = compute_semantic_diversity_loss(semantic_prior)
                
                # æŸå¤±4: è¯­ä¹‰è´¨é‡æŸå¤± (ç¡®ä¿èåˆæ€§å’Œäº’è¡¥æ€§, æƒé‡0.25)
                loss_quality = compute_semantic_quality_loss(
                    semantic_prior, vis_feat, tac_feat
                )
                
                # æ€»æŸå¤±
                total_loss = (1.0 * loss_contrast + 
                            0.3 * loss_recon + 
                            0.2 * loss_diversity + 
                            0.25 * loss_quality)
            
            # åå‘ä¼ æ’­
            if config['use_amp']:
                scaler.scale(total_loss).backward()
                scaler.unscale_(optimizer)
                scaler.unscale_(optimizer_recon)
                torch.nn.utils.clip_grad_norm_(
                    [p for group in trainable_params for p in group['params']], 
                    max_norm=1.0
                )
                scaler.step(optimizer)
                scaler.step(optimizer_recon)
                scaler.update()
            else:
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(
                    [p for group in trainable_params for p in group['params']], 
                    max_norm=1.0
                )
                optimizer.step()
                optimizer_recon.step()
            
            # è®°å½•æŸå¤±
            epoch_losses['contrast'] += loss_contrast.item()
            epoch_losses['reconstruction'] += loss_recon.item()
            epoch_losses['diversity'] += loss_diversity.item()
            epoch_losses['quality'] += loss_quality.item()
            epoch_losses['total'] += total_loss.item()
            
            if batch_idx % 10 == 0:
                lr = optimizer.param_groups[0]['lr']
                all_logger_info(
                    f"[Stage1] Epoch {epoch:3d} Batch {batch_idx:4d} | "
                    f"Contrast: {loss_contrast.item():.4f} | "
                    f"Recon: {loss_recon.item():.4f} | "
                    f"Diversity: {loss_diversity.item():.4f} | "
                    f"Quality: {loss_quality.item():.4f} | "
                    f"LR: {lr:.2e}"
                )
        
        scheduler.step()
        
        # Epochæ€»ç»“
        avg_losses = {k: v / len(train_loader) for k, v in epoch_losses.items()}
        summary = (f"[Stage1] Epoch {epoch:3d}/{config['stage1_epochs']} | "
                  f"Total: {avg_losses['total']:.6f} | "
                  f"Contrast: {avg_losses['contrast']:.6f} | "
                  f"Recon: {avg_losses['reconstruction']:.6f} | "
                  f"Diversity: {avg_losses['diversity']:.6f} | "
                  f"Quality: {avg_losses['quality']:.6f}")
        train_logger_info(summary)
        all_logger_info(summary)
        
        # éªŒè¯é˜¶æ®µ (ä½¿ç”¨å¯¹æ¯”æŸå¤±ä½œä¸ºæŒ‡æ ‡)
        val_loss = validate_stage1(network, test_loader, edge_index, device, config)
        all_logger_info(f"[Stage1] Epoch {epoch:3d} | Val Contrast Loss: {val_loss:.6f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_loss:
            best_loss = val_loss
            patience_counter = 0
            
            if Record_Flag:
                best_path = os.path.join(save_models_path, 'best_semantic_pretrained.pth')
                torch.save({
                    'epoch': epoch,
                    'semantic_extractor': network.semantic_extractor.state_dict(),
                    'imageCNN': network.imageCNN.state_dict(),
                    'tactileGCN': network.tactileGCN.state_dict(),
                    'reconstructor': semantic_reconstructor.state_dict(),
                    'best_loss': best_loss,
                }, best_path)
                all_logger_info(f"âœ“ æ–°æœ€ä½³è¯­ä¹‰æ¨¡å‹ Epoch {epoch}: {best_loss:.6f}")
        else:
            patience_counter += 1
            if patience_counter >= config['patience']:
                all_logger_info(f"[Stage1] æ—©åœè§¦å‘ - {config['patience']} epochsæ— æ”¹å–„")
                break
    
    best_path = os.path.join(save_models_path, 'best_semantic_pretrained.pth')
    return best_path


def validate_stage1(network, test_loader, edge_index, device, config):
    """é˜¶æ®µ1éªŒè¯"""
    network.eval()
    total_contrast_loss = 0.0
    
    with torch.no_grad():
        for batch_idx, data in enumerate(test_loader):
            tip_pose_array, tip_data_array, rgbd_data, target_pose = data
            tip_pose_array = tip_pose_array.to(device, non_blocking=True)
            tip_data_array = tip_data_array.to(device, non_blocking=True)
            rgbd_data = rgbd_data.to(device, non_blocking=True)
            
            batch_graphdata_list = []
            for i, tip_pose in enumerate(tip_pose_array):
                tactile_data = tip_data_array[i]
                graphData = GraphData(x=tip_pose, edge_index=edge_index, 
                                    edge_attr=tactile_data)
                batch_graphdata_list.append(graphData)
            input_tactile_data = Batch.from_data_list(batch_graphdata_list)
            
            with autocast(enabled=config['use_amp']):
                _, vis_feat = network.imageCNN(rgbd_data)
                _, tac_feat = network.tactileGCN(input_tactile_data)
                
                loss_contrast = compute_contrastive_loss(vis_feat, tac_feat)
                total_contrast_loss += loss_contrast.item()
    
    return total_contrast_loss / len(test_loader)


# ==================== é˜¶æ®µ2: å› æœå¾®è°ƒ ====================
def train_stage2_causal_finetuning(network, train_loader, test_loader, 
                                  edge_index, loss6D, config, device, scaler):
    """
    é˜¶æ®µ2: å› æœå¾®è°ƒ
    ç›®æ ‡: åŸºäºé¢„è®­ç»ƒçš„è¯­ä¹‰è¡¨ç¤ºï¼Œè®­ç»ƒå› æœæ¨ç†æ¨¡å—
    """
    print("[Stage2] é…ç½®è®­ç»ƒå‚æ•°...")
    
    # å†³å®šæ˜¯å¦å†»ç»“è¯­ä¹‰æå–å™¨
    freeze_semantic = config['freeze_semantic_stage2']
    
    if freeze_semantic:
        # å†»ç»“è¯­ä¹‰æå–å™¨
        for param in network.semantic_extractor.parameters():
            param.requires_grad = False
        for param in network.imageCNN.parameters():
            param.requires_grad = False
        for param in network.tactileGCN.parameters():
            param.requires_grad = False
        network.semantic_extractor.eval()
        network.imageCNN.eval()
        network.tactileGCN.eval()
        all_logger_info("âœ“ è¯­ä¹‰æå–å™¨å·²å†»ç»“")
    else:
        # å…è®¸å¾®è°ƒï¼Œä½†ä½¿ç”¨æå°å­¦ä¹ ç‡
        for param in network.semantic_extractor.parameters():
            param.requires_grad = True
        for param in network.imageCNN.parameters():
            param.requires_grad = True
        for param in network.tactileGCN.parameters():
            param.requires_grad = True
        all_logger_info("âœ“ è¯­ä¹‰æå–å™¨å°†ä»¥æå°å­¦ä¹ ç‡å¾®è°ƒ")
    
    # è§£å†»å› æœæ¨¡å—å’Œä»»åŠ¡å¤´
    for param in network.csa.parameters():
        param.requires_grad = True
    for param in network.pose_regressor.parameters():
        param.requires_grad = True
    for param in network.fusion_feature.parameters():
        param.requires_grad = True
    
    all_logger_info("âœ“ å› æœæ¨¡å—å’Œä»»åŠ¡å¤´å·²è§£å†»")
    
    # ä¼˜åŒ–å™¨é…ç½®
    if freeze_semantic:
        trainable_params = [
            {'params': network.csa.parameters(), 
             'lr': config['stage2_lr'], 'name': 'csa'},
            {'params': network.pose_regressor.parameters(), 
             'lr': config['stage2_lr'] * 2.5, 'name': 'pose_head'},# åŸæ¥æ˜¯ 1.8
            {'params': network.fusion_feature.parameters(), 
             'lr': config['stage2_lr'], 'name': 'fusion'},
        ]
    else:
        trainable_params = [
            {'params': network.csa.parameters(), 
             'lr': config['stage2_lr'], 'name': 'csa'},
            {'params': network.pose_regressor.parameters(), 
             'lr': config['stage2_lr'] * 2.5, 'name': 'pose_head'},# åŸæ¥æ˜¯ 1.8
            {'params': network.fusion_feature.parameters(), 
             'lr': config['stage2_lr'], 'name': 'fusion'},
            {'params': network.semantic_extractor.parameters(), 
             'lr': config['stage2_lr'] * 0.01, 'name': 'semantic'},
            {'params': network.imageCNN.parameters(), 
             'lr': config['stage2_lr'] * 0.01, 'name': 'imageCNN'},
            {'params': network.tactileGCN.parameters(), 
             'lr': config['stage2_lr'] * 0.01, 'name': 'tactileGCN'},
        ]
    
    optimizer = torch.optim.AdamW(trainable_params, 
                                 weight_decay=config['stage2_weight_decay'],
                                 betas=(0.9, 0.999), eps=1e-8)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ (Warmup + Cosine)
    warmup_epochs = 15   # â† ä» 10 â†’ 20 (æ›´é•¿çš„ warmup)
    total_steps = len(train_loader) * config['stage2_epochs']
    warmup_steps = len(train_loader) * warmup_epochs
    
    def lr_lambda(current_step):
        if current_step < warmup_steps:
            return float(current_step) / float(max(1, warmup_steps))
        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))
        # ğŸ¯ å…³é”®æ”¹åŠ¨: æœ€å°å­¦ä¹ ç‡æ›´é«˜ï¼Œè¡°å‡æ›´ç¼“æ…¢
        return max(0.03, 0.5 * (1.0 + math.cos(math.pi * progress)))
        # â† æœ€å°å­¦ä¹ ç‡ä» 0.01 â†’ 0.08
        # â† progress * 0.8 ä½¿è¡°å‡å˜æ…¢
    
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    
    # è®­ç»ƒç›‘æ§
    best_loss = float('inf')
    patience_counter = 0
    global_step = 0
    
    # åŠ¨æ€æŸå¤±æƒé‡
    loss_weight_manager = DynamicLossWeightManager()

    # ========== æ–°å¢è¿™1è¡Œ ==========
    adaptive_weights = SimpleAdaptiveWeights()
    # ========== æ–°å¢ç»“æŸ ==========
    
    # ==================== è®­ç»ƒå¾ªç¯ ====================
    for epoch in range(config['stage2_epochs']):
        if freeze_semantic:
            network.semantic_extractor.eval()
            network.imageCNN.eval()
            network.tactileGCN.eval()
            network.csa.train()
            network.pose_regressor.train()
            network.fusion_feature.train()
        else:
            network.train()
        
        # æ¸è¿›å¼è§£å†»LLM
        if epoch == config['stage2_llm_unfreeze_epoch'] and not freeze_semantic:
            all_logger_info(f"\n>>> Epoch {epoch}: å¼€å§‹è§£å†»LLMæœ€å2å±‚")
            unfreeze_llm_layers(network, num_layers=2)
            # å¢åŠ LLMå­¦ä¹ ç‡
            for group in optimizer.param_groups:
                if 'semantic' in group['name']:
                    group['lr'] = config['stage2_lr'] * 0.02
        
        epoch_losses = {
            'pose': 0.0,
            'causal_reg': 0.0,
            'trans': 0.0,      # â† æ–°å¢
            'rot': 0.0,        # â† æ–°å¢
            'total': 0.0
        }
        
        for batch_idx, data in enumerate(train_loader):
            tip_pose_array, tip_data_array, rgbd_data, target_pose = data
            tip_pose_array = tip_pose_array.to(device, non_blocking=True)
            tip_data_array = tip_data_array.to(device, non_blocking=True)
            rgbd_data = rgbd_data.to(device, non_blocking=True)
            target_pose = target_pose.to(device, non_blocking=True)
            
            # æ„å»ºå›¾æ•°æ®
            batch_graphdata_list = []
            for i, tip_pose in enumerate(tip_pose_array):
                tactile_data = tip_data_array[i]
                graphData = GraphData(x=tip_pose, edge_index=edge_index, 
                                    edge_attr=tactile_data)
                batch_graphdata_list.append(graphData)
            input_tactile_data = Batch.from_data_list(batch_graphdata_list)
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            with autocast(enabled=config['use_amp']):
                if freeze_semantic:
                    with torch.no_grad():
                        _, vis_feat = network.imageCNN(rgbd_data)
                        _, tac_feat = network.tactileGCN(input_tactile_data)
                        semantic_prior = network.semantic_extractor(vis_feat, tac_feat)
                else:
                    _, vis_feat = network.imageCNN(rgbd_data)
                    _, tac_feat = network.tactileGCN(input_tactile_data)
                    semantic_prior = network.semantic_extractor(vis_feat, tac_feat)
                
                # å› æœå¤„ç†
                causal_feature, causal_info = network.csa(
                    vis_feat, tac_feat, semantic_prior
                )
                
                # å§¿æ€é¢„æµ‹
                predict_pose = network.pose_regressor(causal_feature)
                
                # æŸå¤±è®¡ç®—
                # loss_pose = loss6D(predict_pose, target_pose) ---(åŸæ¥æ˜¯è¿™æ ·)

                # ========== ä¿®æ”¹åçš„ä»£ç  ==========
                loss_translation, loss_rotation = loss6D.compute_separate(
                    predict_pose, target_pose
                )

                # ğŸ”¥ ä½¿ç”¨è‡ªé€‚åº”æƒé‡
                trans_weight, rot_weight = adaptive_weights.update(
                    loss_translation.item(),  # ä¼ å…¥æŸå¤±çš„æ•°å€¼
                    loss_rotation.item()
                )

                loss_pose = trans_weight * loss_translation + rot_weight * loss_rotation
                # ========== ä¿®æ”¹ç»“æŸ ==========
                #########################################################
                
                loss_causal = compute_causal_regularization_enhanced(
                    network, rgbd_data, input_tactile_data
                )
                
                # åŠ¨æ€æƒé‡
                weights = loss_weight_manager.get_weights(epoch)
                total_loss = loss_pose + weights['causal'] * loss_causal
            
            # åå‘ä¼ æ’­
            if config['use_amp']:
                scaler.scale(total_loss).backward()
                scaler.unscale_(optimizer)
                # åˆ†ç»„æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(network.csa.parameters(), max_norm=1.0)
                torch.nn.utils.clip_grad_norm_(network.pose_regressor.parameters(), max_norm=2.8)
                if not freeze_semantic:
                    torch.nn.utils.clip_grad_norm_(
                        network.semantic_extractor.llava_model.parameters(), max_norm=0.5
                    )
                scaler.step(optimizer)
                scaler.update()
            else:
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(network.csa.parameters(), max_norm=1.0)
                torch.nn.utils.clip_grad_norm_(network.pose_regressor.parameters(), max_norm=2.8)
                optimizer.step()
            
            scheduler.step()
            global_step += 1
            
            # è®°å½•æŸå¤±
            epoch_losses['pose'] += loss_pose.item()
            epoch_losses['trans'] += loss_translation.item()   # â† æ–°å¢
            epoch_losses['rot'] += loss_rotation.item()        # â† æ–°å¢
            epoch_losses['causal_reg'] += loss_causal.item()
            epoch_losses['total'] += total_loss.item()
            
            # ğŸ¯ ä¿®æ”¹ç‚¹4: æ‰“å°ä¿¡æ¯ (æ·»åŠ ä½ç½®å’Œè§’åº¦çš„è¯¦ç»†ä¿¡æ¯)
            if batch_idx % 10 == 0:
                lr = optimizer.param_groups[0]['lr']
                all_logger_info(
                    f"[Stage2] Epoch {epoch:3d} Batch {batch_idx:4d} | "
                    f"Trans: {loss_translation.item():.6f}(Ã—{trans_weight}) | "
                    f"Rot: {loss_rotation.item():.6f}(Ã—{rot_weight}) | "
                    f"Causal: {loss_causal.item():.4f}(w={weights['causal']:.3f}) | "
                    f"LR: {lr:.2e}"
                )
        
        # ğŸ¯ ä¿®æ”¹ç‚¹5: Epochæ€»ç»“ (æ·»åŠ ä½ç½®å’Œè§’åº¦ç»Ÿè®¡)
        avg_losses = {k: v / len(train_loader) for k, v in epoch_losses.items()}
        loss_weight_manager.update_history(avg_losses['pose'], avg_losses['causal_reg'])
        
        summary = (f"[Stage2] Epoch {epoch:3d}/{config['stage2_epochs']} | "
                  f"Total: {avg_losses['total']:.6f} | "
                  f"Trans: {avg_losses['trans']:.6f} | "   # â† æ–°å¢
                  f"Rot: {avg_losses['rot']:.6f} | "       # â† æ–°å¢
                  f"Causal: {avg_losses['causal_reg']:.6f}")
        train_logger_info(summary)
        all_logger_info(summary)
        
        # éªŒè¯é˜¶æ®µ
        val_loss = validate_stage2(network, test_loader, edge_index, 
                                loss6D, device, config, freeze_semantic, adaptive_weights)
        all_logger_info(f"[Stage2] Epoch {epoch:3d} | Val Pose Loss: {val_loss:.6f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_loss:
            best_loss = val_loss
            patience_counter = 0
            
            if Record_Flag:
                best_path = os.path.join(save_models_path, 'best_model.pth')
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': network.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'scaler_state_dict': scaler.state_dict() if scaler else None,
                    'best_loss': best_loss,
                    'config': config,
                }, best_path)
                all_logger_info(f"âœ“ æ–°æœ€ä½³æ¨¡å‹ Epoch {epoch}: {best_loss:.6f}")
        else:
            patience_counter += 1
            if patience_counter >= config['patience']:
                all_logger_info(f"[Stage2] æ—©åœè§¦å‘ - {config['patience']} epochsæ— æ”¹å–„")
                break
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if Record_Flag and epoch % 50 == 0:
            checkpoint_path = os.path.join(save_models_path, f'checkpoint_stage2_epoch{epoch}.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': network.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': val_loss,
            }, checkpoint_path)
    
    # æœ€ç»ˆä¿å­˜
    if Record_Flag:
        last_path = os.path.join(save_models_path, 'last_model.pth')
        torch.save(network.state_dict(), last_path)
        all_logger_info(f"âœ“ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {last_path}")
    
    return best_loss


def validate_stage2(network, test_loader, edge_index, loss6D, device, config, freeze_semantic, adaptive_weights=None):
    """é˜¶æ®µ2éªŒè¯"""
    if freeze_semantic:
        network.semantic_extractor.eval()
        network.imageCNN.eval()
        network.tactileGCN.eval()
    network.csa.eval()
    network.pose_regressor.eval()
    network.fusion_feature.eval()
    
    total_pose_loss = 0.0
    # æ–°å¢è¿™ä¸¤è¡Œ
    total_trans_loss = 0.0  
    total_rot_loss = 0.0
    
    with torch.no_grad():
        for batch_idx, data in enumerate(test_loader):
            tip_pose_array, tip_data_array, rgbd_data, target_pose = data
            tip_pose_array = tip_pose_array.to(device, non_blocking=True)
            tip_data_array = tip_data_array.to(device, non_blocking=True)
            rgbd_data = rgbd_data.to(device, non_blocking=True)
            target_pose = target_pose.to(device, non_blocking=True)
            
            batch_graphdata_list = []
            for i, tip_pose in enumerate(tip_pose_array):
                tactile_data = tip_data_array[i]
                graphData = GraphData(x=tip_pose, edge_index=edge_index, 
                                    edge_attr=tactile_data)
                batch_graphdata_list.append(graphData)
            input_tactile_data = Batch.from_data_list(batch_graphdata_list)
            
            with autocast(enabled=config['use_amp']):
                _, vis_feat = network.imageCNN(rgbd_data)
                _, tac_feat = network.tactileGCN(input_tactile_data)
                semantic_prior = network.semantic_extractor(vis_feat, tac_feat)
                causal_feature, _ = network.csa(vis_feat, tac_feat, semantic_prior)
                predict_pose = network.pose_regressor(causal_feature)
                
                # ğŸ¯ æ ¸å¿ƒæ”¹åŠ¨: ä½¿ç”¨åˆ†ç¦»æŸå¤±
                trans_loss, rot_loss = loss6D.compute_separate(predict_pose, target_pose)
                
                # ğŸ”¥ ä½¿ç”¨è‡ªé€‚åº”æƒé‡ï¼ˆå¦‚æœæä¾›çš„è¯ï¼‰
                if adaptive_weights is not None:
                    trans_weight = adaptive_weights.trans_w
                    rot_weight = adaptive_weights.rot_w
                else:
                    # å¦‚æœæ²¡æœ‰æä¾›ï¼Œä½¿ç”¨é»˜è®¤æƒé‡
                    trans_weight = 1.2
                    rot_weight = 1.0
                pose_loss = trans_weight * trans_loss + rot_weight * rot_loss
                total_pose_loss += pose_loss.item()
                total_trans_loss += trans_loss.item()
                total_rot_loss += rot_loss.item()
                #########################################################

                # loss_pose = loss6D(predict_pose, target_pose) ----(åŸæ¥æ˜¯è¿™æ ·)
    
    return total_pose_loss / len(test_loader)


# ==================== è¾…åŠ©ç±»å’Œå‡½æ•° ====================

class SemanticReconstructor(nn.Module):
    """è¯­ä¹‰é‡æ„å™¨ - ç”¨äºè¯„ä¼°è¯­ä¹‰å…ˆéªŒçš„ä¿¡æ¯ä¿ç•™èƒ½åŠ›"""
    def __init__(self, semantic_dim=768, feature_dim=128):
        super().__init__()
        
        # é‡æ„è§†è§‰ç‰¹å¾
        self.vis_decoder = nn.Sequential(
            nn.Linear(semantic_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, feature_dim)
        )
        
        # é‡æ„è§¦è§‰ç‰¹å¾
        self.tac_decoder = nn.Sequential(
            nn.Linear(semantic_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, feature_dim)
        )
    
    def forward(self, semantic_prior):
        """
        ä»è¯­ä¹‰å…ˆéªŒé‡æ„è§†è§‰å’Œè§¦è§‰ç‰¹å¾
        Args:
            semantic_prior: [B, 768]
        Returns:
            vis_reconstructed: [B, 128]
            tac_reconstructed: [B, 128]
        """
        vis_recon = self.vis_decoder(semantic_prior)
        tac_recon = self.tac_decoder(semantic_prior)
        return vis_recon, tac_recon


class DynamicLossWeightManager:
    """åŠ¨æ€æŸå¤±æƒé‡ç®¡ç†å™¨"""
    def __init__(self):
        self.pose_history = []
        self.causal_history = []
        self.base_causal_weight = 0.2
    
    def update_history(self, pose_loss, causal_loss):
        """æ›´æ–°æŸå¤±å†å²"""
        self.pose_history.append(pose_loss)
        self.causal_history.append(causal_loss)
        
        # åªä¿ç•™æœ€è¿‘20ä¸ªepochçš„å†å²
        if len(self.pose_history) > 20:
            self.pose_history.pop(0)
            self.causal_history.pop(0)
    
    def get_weights(self, epoch):
        """
        è·å–åŠ¨æ€æƒé‡
        ç­–ç•¥:
        1. åŸºç¡€æƒé‡éšepochçº¿æ€§å¢åŠ 
        2. å¦‚æœå§¿æ€æŸå¤±ä¸‹é™åœæ»,å¢åŠ å› æœæƒé‡
        3. å¦‚æœå§¿æ€æŸå¤±ä¸‹é™è¿‡å¿«,å‡å°‘å› æœæƒé‡
        """
        # ç­–ç•¥1: åŸºç¡€å¢é•¿
        causal_weight = min(self.base_causal_weight + 0.015 * (epoch / 10), 0.7)
        
        # ç­–ç•¥2: è‡ªé€‚åº”è°ƒæ•´
        if len(self.pose_history) >= 5:
            recent_improvement = self.pose_history[-5] - self.pose_history[-1]
            
            if recent_improvement < 0.0005:  # æ”¹å–„ä¸æ˜æ˜¾
                causal_weight = min(causal_weight * 1.2, 0.8)
            elif recent_improvement > 0.005:  # æ”¹å–„å¾ˆå¿«
                causal_weight = max(causal_weight * 0.9, 0.15)
        
        return {
            'causal': causal_weight
        }


def compute_contrastive_loss(vis_feat, tac_feat, temperature=0.07):
    """
    å¯¹æ¯”å­¦ä¹ æŸå¤± - InfoNCE
    Args:
        vis_feat: [B, 128] è§†è§‰ç‰¹å¾
        tac_feat: [B, 128] è§¦è§‰ç‰¹å¾
        temperature: æ¸©åº¦å‚æ•°
    Returns:
        contrast_loss: å¯¹æ¯”æŸå¤±
    """
    # L2å½’ä¸€åŒ–
    vis_feat = F.normalize(vis_feat, dim=-1)
    tac_feat = F.normalize(tac_feat, dim=-1)
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    logits = torch.matmul(vis_feat, tac_feat.T) / temperature  # [B, B]
    
    # æ ‡ç­¾: å¯¹è§’çº¿ä¸ºæ­£æ ·æœ¬
    batch_size = vis_feat.shape[0]
    labels = torch.arange(batch_size, device=vis_feat.device)
    
    # åŒå‘å¯¹æ¯”æŸå¤±
    loss_v2t = F.cross_entropy(logits, labels)
    loss_t2v = F.cross_entropy(logits.T, labels)
    
    contrast_loss = (loss_v2t + loss_t2v) / 2
    
    return contrast_loss


def compute_semantic_diversity_loss(semantic_prior):
    """
    è¯­ä¹‰å¤šæ ·æ€§æŸå¤± - é˜²æ­¢mode collapse
    ç¡®ä¿batchå†…çš„è¯­ä¹‰å…ˆéªŒæœ‰è¶³å¤Ÿå·®å¼‚
    """
    batch_size = semantic_prior.shape[0]
    
    if batch_size <= 1:
        return torch.tensor(0.0, device=semantic_prior.device)
    
    # L2å½’ä¸€åŒ–
    semantic_norm = F.normalize(semantic_prior, dim=-1)
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    similarity_matrix = torch.matmul(semantic_norm, semantic_norm.T)  # [B, B]
    
    # åˆ›å»ºmask (æ’é™¤å¯¹è§’çº¿)
    mask = 1 - torch.eye(batch_size, device=semantic_prior.device)
    
    # æƒ©ç½šè¿‡é«˜çš„ç›¸ä¼¼åº¦
    # ç›®æ ‡: batchå†…æ ·æœ¬ç›¸ä¼¼åº¦åº”è¯¥è¾ƒä½
    off_diagonal_sim = similarity_matrix * mask
    diversity_loss = off_diagonal_sim.pow(2).sum() / (batch_size * (batch_size - 1))
    
    return diversity_loss


def compute_semantic_quality_loss(semantic_prior, vis_feat, tac_feat):
    """
    è¯­ä¹‰è´¨é‡æŸå¤± - ç¡®ä¿è¯­ä¹‰å…ˆéªŒæœ‰æ„ä¹‰ä¸”ä¸é€€åŒ–
    
    è¯„ä¼°æ ‡å‡†:
    1. èåˆæ€§: è¯­ä¹‰å…ˆéªŒåº”ä¸è§†è§‰å’Œè§¦è§‰éƒ½ç›¸å…³ (ç›¸ä¼¼åº¦0.4-0.6)
    2. äº’è¡¥æ€§: ä¸åº”è¿‡åº¦åå‘æŸä¸€æ¨¡æ€
    3. æŠ½è±¡æ€§: ä¸åº”é€€åŒ–ä¸ºè¾“å…¥ç‰¹å¾çš„ç®€å•å¤åˆ¶
    """
    batch_size = semantic_prior.shape[0]
    semantic_dim = semantic_prior.shape[-1]  # 768
    feature_dim = vis_feat.shape[-1]  # 128
    
    # ğŸ”§ ä¿®å¤: æ·»åŠ ä¸´æ—¶æŠ•å½±å±‚ï¼Œå°†semantic_prioræŠ•å½±åˆ°featureç©ºé—´
    if semantic_dim != feature_dim:
        # ä½¿ç”¨ç®€å•çš„çº¿æ€§æŠ•å½± (ä¸å‚ä¸æ¢¯åº¦æ›´æ–°ï¼Œä»…ç”¨äºç›¸ä¼¼åº¦è®¡ç®—)
        projection = nn.Linear(semantic_dim, feature_dim, bias=False).to(semantic_prior.device)
        # ä½¿ç”¨Xavieråˆå§‹åŒ–
        nn.init.xavier_uniform_(projection.weight)
        with torch.no_grad():
            semantic_projected = projection(semantic_prior)
    else:
        semantic_projected = semantic_prior
    
    # å½’ä¸€åŒ–
    semantic_norm = F.normalize(semantic_projected, dim=-1)
    vis_norm = F.normalize(vis_feat, dim=-1)
    tac_norm = F.normalize(tac_feat, dim=-1)
    
    # 1. èåˆæ€§çº¦æŸ
    vis_sim = F.cosine_similarity(semantic_norm, vis_norm, dim=-1).mean()
    tac_sim = F.cosine_similarity(semantic_norm, tac_norm, dim=-1).mean()
    
    # ç›®æ ‡ç›¸ä¼¼åº¦: 0.5 (å¤ªé«˜=é€€åŒ–, å¤ªä½=æ— å…³)
    fusion_loss = (torch.abs(vis_sim - 0.5) + torch.abs(tac_sim - 0.5)) / 2
    
    # 2. äº’è¡¥æ€§çº¦æŸ (ä¸¤ä¸ªæ¨¡æ€çš„ç›¸ä¼¼åº¦åº”æ¥è¿‘)
    balance_loss = torch.abs(vis_sim - tac_sim)
    
    # 3. æŠ½è±¡æ€§çº¦æŸ (è¯­ä¹‰å…ˆéªŒç»´åº¦æ›´é«˜,æŠ•å½±åˆ°ä½ç»´åº”æœ‰ä¿¡æ¯æŸå¤±)
    if batch_size > 1:
        # è®¡ç®—åæ–¹å·®çŸ©é˜µçš„è¿¹ (ä¿¡æ¯é‡çš„ç²—ç•¥ä¼°è®¡)
        semantic_centered = semantic_prior - semantic_prior.mean(dim=0, keepdim=True)
        cov = torch.matmul(semantic_centered.T, semantic_centered) / (batch_size - 1)
        trace = torch.trace(cov)
        
        # æƒ©ç½šè¿‡ä½çš„ä¿¡æ¯é‡ (ä½äºé˜ˆå€¼)
        abstraction_loss = F.relu(100.0 - trace) / 100.0
    else:
        abstraction_loss = torch.tensor(0.0, device=semantic_prior.device)
    
    # ç»„åˆæŸå¤±
    total_quality_loss = fusion_loss + 0.5 * balance_loss + 0.3 * abstraction_loss
    
    return total_quality_loss


def compute_causal_regularization_enhanced(network, rgbd_data, tactile_data):
    """
    å¢å¼ºç‰ˆå› æœæ­£åˆ™åŒ–æŸå¤±
    åŒ…å«: ç¨€ç–æ€§ + åäº‹å®ä¸€è‡´æ€§ + ç‹¬ç«‹æ€§ + ä¸å˜æ€§
    """
    # æå–å› æœæ¨¡å—çš„ä¸­é—´ç‰¹å¾
    _, vis_feat = network.imageCNN(rgbd_data)
    _, tac_feat = network.tactileGCN(tactile_data)
    semantic_prior = network.semantic_extractor(vis_feat, tac_feat)
    
    # è·å–å› æœåˆ†æä¿¡æ¯
    _, causal_info = network.csa(vis_feat, tac_feat, semantic_prior)
    
    # 1. è™šå‡ç‰¹å¾ç¨€ç–æ€§çº¦æŸ
    spurious_loss = (torch.norm(causal_info['vis_spurious'], p=1) + 
                     torch.norm(causal_info['tac_spurious'], p=1)) / 2
    
    # 2. åäº‹å®ä¸€è‡´æ€§çº¦æŸ
    counterfactual_loss = (causal_info['counterfactual_gap_vis'] + 
                           causal_info['counterfactual_gap_tac']) / 2
    
    # 3. å› æœç‰¹å¾ç‹¬ç«‹æ€§çº¦æŸ
    vis_causal = causal_info['vis_causal']
    tac_causal = causal_info['tac_causal']
    independence_loss = torch.abs(F.cosine_similarity(
        vis_causal, tac_causal, dim=-1
    )).mean()
    
    # 4. å› æœä¸å˜æ€§çº¦æŸ
    vis_causal_norm = F.normalize(vis_causal, dim=-1)
    vis_spurious_norm = F.normalize(causal_info['vis_spurious'], dim=-1)
    tac_causal_norm = F.normalize(tac_causal, dim=-1)
    tac_spurious_norm = F.normalize(causal_info['tac_spurious'], dim=-1)
    
    invariance_loss = (
        torch.abs(F.cosine_similarity(vis_causal_norm, vis_spurious_norm, dim=-1)).mean() +
        torch.abs(F.cosine_similarity(tac_causal_norm, tac_spurious_norm, dim=-1)).mean()
    ) / 2
    
    # ç»„åˆæŸå¤±
    total_causal_reg = (0.25 * spurious_loss + 
                       0.35 * counterfactual_loss + 
                       0.2 * independence_loss + 
                       0.2 * invariance_loss)
    
    return total_causal_reg


def unfreeze_llm_layers(network, num_layers=2):
    """
    æ¸è¿›å¼è§£å†»LLaVAçš„Transformerå±‚
    Args:
        network: ä¸»ç½‘ç»œ
        num_layers: è§£å†»æœ€åNå±‚
    """
    llava_model = network.semantic_extractor.llava_model
    
    if hasattr(llava_model, 'language_model'):
        language_model = llava_model.language_model
        if hasattr(language_model, 'model') and hasattr(language_model.model, 'layers'):
            layers = language_model.model.layers
        elif hasattr(language_model, 'layers'):
            layers = language_model.layers
        else:
            all_logger_info("  è­¦å‘Š: æ— æ³•å®šä½Transformerå±‚")
            return
        
        # è§£å†»æœ€ånum_layerså±‚
        total_layers = len(layers)
        unfrozen_count = 0
        for i in range(max(0, total_layers - num_layers), total_layers):
            for param in layers[i].parameters():
                param.requires_grad = True
                unfrozen_count += 1
        
        all_logger_info(f"  âœ“ å·²è§£å†»LLMæœ€å {num_layers} å±‚ (å…±{total_layers}å±‚, {unfrozen_count}ä¸ªå‚æ•°)")
    else:
        all_logger_info("  è­¦å‘Š: æ— æ³•è®¿é—®language_model")